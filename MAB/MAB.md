# MAB (Multi-Armed Bandit)

개인화 추천 기술 : 개인마다 각 콘텐츠에 기대할 수 있는 클릭률을 계산하고 그 확률이 가장 높은 콘텐츠를 맞춤형으로 제시해주는 것을 말합니다.\
\
### MAB란 ?
강화 학습 문제의 일종으로, 보상을 알 수 없는 K개의 선택지가 존재할 때 탐색 (Exploration)과 활용 (Exploitation)을 적절히 조절하여 최종 보상의 총합을 최대화하는 문제입니다.\
\
따라서 MAB의 최종 목표는 "어떤 정책 (Policy)를 통해 최대의 보상 (reward)을 얻을 것인가" 라고 할 수 있습니다 :)

이때 **탐색 (Exploration)**은 사용자에게 어떤 콘텐츠를 추천할지 탐색하기 위해 새로운 콘텐츠를 추천해주고 피드백을 수집하는 과정을 의미합니다.

**활용 (Exploitation)**은 피드백을 바탕으로 사용자가 좋아할 것으로 예상되는 콘텐츠를 추천해주는 과정을 의미합니다.


# MAB의 기본적인 수식

![1](https://github.com/DEVOCEAN-YOUNG-DEVSHIP/recsys-study/assets/132445323/0d3c36dd-39f1-4a1d-84e0-29e677e09509)


# MAB를 사용하는 이유

#### **1) 실제 환경에서 목표 (Objective)를 학습할 수 있다 !**
- 데이터 x를 바탕으로 유저 u의 아이템 i에 대한 선호도 측정. 즉, y_u, i가 목표 (objective)가 됩니다. 이는 평점, 클릭 여부, 체류 시간 등이 될 수 있습니다.
- 과거 데이터를 바탕으로 지도 학습 협업 필터링 모델을 학습할 경우, 바로 피드백을 확인할 수 없기 때문에 대리 목표 (Surrogative objective)를 설정해야 합니다.
- 따라서 실제 현업에서 클릭률 / 구매율 등을 최적화하고자 할 때 실제 목표와의 간극이 발생할 수 있습니다. 
- MAB의 경우, 에이전트가 실제 환경에서 직접 상호작용을 하며 피드백을 학습할 수 있습니다.
- MAB는 유저 u에게 아이템 i를 추천해주고, 그에 대한 실제 피드백을 관찰, 업데이트할 수 있습니다.

#### **2) 장기적인 보상을 최적화해주는 탐색 과정 !**
- T번의 time step 동안 action을 취하는 일종의 MDP (Markov Decision Process)하에서 누적 보상을 최대화하도록 설계된 알고리즘입니다.
- 탐색 과정에서 현재 스탭에서 기대 보상이 높지 않더라도, 이후 미래 보상을 높여줄 수 있다면 해당 행동을 선택할 수 있습니다.
- 익숙하지 않지만 좋아할 수 있는 아이템 추천이 가능합니다 !

#### **3) 실시간성 !**
- 사용자의 피드백이 실시간으로 모델에게 적용되어 추천 결과에 나타납니다.
- 추천 결과를 보여주고 그에 대한 사용자의 피드백이 모델에 반영되어 다시 추천 결과에 나타나는 한번의 루프에 드는 시간이 얼마나 짧은지에 따라 실제 클릭률이 유의미하게 달라진다고 합니다.


# MAB의 기초 알고리즘

![5](https://github.com/DEVOCEAN-YOUNG-DEVSHIP/recsys-study/assets/132445323/dfdc4348-62da-4674-be4a-178eae641bd4)

**U자 형태** : exploration과 exploitation이 적절히 trade-off 되는 시점을 찾아야 하는 과제.

#### **1) Greedy Algorithm **

<img width="838" alt="2" src="https://github.com/DEVOCEAN-YOUNG-DEVSHIP/recsys-study/assets/132445323/0084c81e-d2a2-46ea-baa0-59aa435447c5">

- action에서 얻은 reward의 단순 평균.
- Qt(a)가 계속 업데이트되다가 평균 reward가 최대인 action을 선택합니다.
- 처음에 선택되는 action이 계속 높은 reward를 주다가 우연히 낮은 reward를 준다면 그 action의 reward 평균이 낮아져 다시는 선택되지 않는다는 문제점이 있습니다.

#### **2) ε-greedy Algorithm **

<img width="867" alt="3" src="https://github.com/DEVOCEAN-YOUNG-DEVSHIP/recsys-study/assets/132445323/15026133-9319-405d-94d0-228c39f06f00">

- ε의 일정한 비율로 랜덤한 Arm을 선택 (탐색 과정) + 1 - ε 비율로 보상이 가장 좋은 Arm을 선택 (활용 과정)
- 각 대상이 공평한 기회를 얻고 간단한 방식으로 속도를 빠르게 낼 수 있습니다.
- 탐색을 위해 ε 만큼의 리소스가 소모되며 데이터가 많이 쌓여 true distribution을 측정했다하더라도 계속해서 ε의 확률로 랜덤하게 선택하기 때문에 후반에는 손해를 보게 된다는 문제점이 있습니다.

#### **3) UCB (Upper Confidence Bound) Algorithm **

<img width="805" alt="4" src="https://github.com/DEVOCEAN-YOUNG-DEVSHIP/recsys-study/assets/132445323/e7ff24d0-8141-485e-9a34-c7b58625c9da">

- ε-greedy algorithm의 튜닝 버전.
- 자연스럽게 exploration 에서 exploitation으로 policy 변화를 이끌 수 있습니다.
- 계산이 까다로워 연산 비용이 크고, 연산 이후 최고의 조건에서 활용이 이루어져 환경적 요인이 반영되기 어렵다는 문제점이 있습니다.

#### **4) Thompson Sampling **

![6](https://github.com/DEVOCEAN-YOUNG-DEVSHIP/recsys-study/assets/132445323/95dba8bb-6b3a-48ad-b41e-0e7655781c6c)

- 가장 대표적인 알고리즘.
- K개의 Arm 중 각 Arm은 하나의 콘텐츠, Success는 클릭을 의미합니다. 따라서 어떤 Arm을 선택하는 것은 해당 콘텐츠를 추천하는 것을 의미합니다.
- 베이지안 통계를 기반으로 하여 각 Arm의 성공 확률을 베타 분포를 따르는 확률 변수로 보고, 처음에는 동일한 파라미터를 가지는 베타 분포로 초기화를 합니다.
- 매 타임 스텝마다 Arm의 확률분포로부터 예상 클릭률을 샘플링한 후, 값이 가장 높은 Arm을 선택하고, 그에 대한 reward를 바탕으로 베타 분포가 업데이트되는 형식입니다.
- 과거 성과에 기반하여 성공/실패 확률 분포를 추론하는 과정이기 때문에 확률을 기반으로 탐색과 활용을 분배할 수 있습니다.


# 카카오의 사례 - 토픽 모델링과 MAB를 통한 추천 시스템

#### ** 추천이 나가기 전 **

![7](https://github.com/DEVOCEAN-YOUNG-DEVSHIP/recsys-study/assets/132445323/cbfea69f-e3fe-47e7-a60c-4d3167c36319)

* 사전에 쌓아놓은 사용자와 아이템 간의 클릭 로그를 기반으로 토픽 모델링을 진행합니다.
    * 사용자와 아이템 각각에 대한 주제 벡터 계산 !
    * 사용자 주제 벡터 : 사용자가 각 주제를 얼마나 선호하는지.
    * 아이템 주제 벡터 : 아이템이 각 주제에 얼마나 속하는지.
    * 각 주제 별로 MAB를 하나씩 할당합니다.

* 사진 : 사용자와 아이템이 [패스트푸드, 제과, 육류]라는 주제에 얼마나 속하고 있는지 계산합니다.
* 사용자의 주제 벡터가 [0.06, 0.10, 0.84]로 구성. (사용자는 육류에 가장 관심을 가지고 있음)
* 그와 동시에 각 주제에 해당하는 MAB에서는 모든 아이템에 대해 각 주제에 맞는 추천 점수를 가집니다.

#### ** 추천 요청이 들어올 경우 **

![8](https://github.com/DEVOCEAN-YOUNG-DEVSHIP/recsys-study/assets/132445323/0abd82d6-0737-4639-951b-3ff8e7dfc4c8)

* 주제별 MAB들의 추천 점수를 사용자의 주제 벡터에 맞게 가중 평균하여 계산합니다.
* 이때, 사용자의 취향이 '육류'에 치우쳐 있다면, 추천 결과 또한 '육류' MAB의 추천 결과에 가중하여 전달할 것입니다.

#### ** 피드백을 통한 실시간 최적화 **

* 추천이 나갈 때와 유사하게 피드백 또한 주제별로 속할 확률을 바탕으로 각 MAB에 피드백을 나누어서 전달합니다.
    * 피드백에 해당하는 추천이 ‘패스트 푸드’라는 주제에 집중했던 추천이라면, 해당 추천의 피드백 또한 ‘패스트푸드’의 MAB에 집중되어 전달되는 것입니다 !